{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e420d98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47791f9f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43e00d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from configparser import ConfigParser\n",
    "from pathlib import Path\n",
    "import pyspark.sql.functions as F\n",
    "#import requests\n",
    "from pyspark.sql.types import ArrayType, StringType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fbb4ea",
   "metadata": {},
   "source": [
    "### Define directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c0e75ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "#\n",
    "# Relevant directories are read from the config file:\n",
    "# dir_data:    full path to hdfs directory where the raw data .gz files are stored\n",
    "# dir_parquet: full path to hdfs directory where the parquet tables will be stored\n",
    "# version:     Version of Semantic Scholar that is being processed\n",
    "#              for information purposes only\n",
    "\n",
    "# cf = ConfigParser()\n",
    "# cf.read(\"../config.cf\")\n",
    "\n",
    "# dir_data = Path(cf.get(\"spark\", \"dir_data\"))\n",
    "# dir_parquet = Path(cf.get(\"spark\", \"dir_parquet\"))\n",
    "# version = cf.get(\"spark\", \"version\")\n",
    "# dir_pdfs = Path(cf.get(\"spark\", \"dir_pdfs\"))\n",
    "\n",
    "dir_data = Path('/export/clusterdata/jarenas/Datasets/semanticscholar/20230418/rawdata')\n",
    "dir_parquet = Path('/export/ml4ds/IntelComp/Datalake/semanticscholar/20230418/parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b88ac9d",
   "metadata": {},
   "source": [
    "### Configuration hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1c1fc6",
   "metadata": {},
   "source": [
    "It is not possible to listdir() directly using Path as it is a hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f7223f0-13c1-4c23-a3e0-6a59fd7e881c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration hdfs\n",
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "hdfs_dir_parquet = spark._jvm.org.apache.hadoop.fs.Path(dir_parquet.as_posix())\n",
    "# Create output directories if they do not exist\n",
    "# !hadoop dfs ...\n",
    "# !hadoop dfs -put 20220201 /export/ml4ds/IntelComp/Datalake/SemanticScholar/\n",
    "\n",
    "if not fs.exists(hdfs_dir_parquet):\n",
    "    fs.mkdirs(hdfs_dir_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11773d79-bdbd-4c85-a37a-3b169bca673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Configuration hdfs\n",
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "hdfs_dir_data = spark._jvm.org.apache.hadoop.fs.Path(dir_data.as_posix())\n",
    "\n",
    "print(hdfs_dir_data)\n",
    "\n",
    "# Get selected version\n",
    "releases = sorted(\n",
    "    [\n",
    "        f.getPath().getName()\n",
    "        for f in fs.listStatus(hdfs_dir_data)\n",
    "        if f.isDirectory() and f.getPath().getName().isdigit()\n",
    "    ]\n",
    ")\n",
    "version = version.replace(\"-\", \"\")\n",
    "if version == \"last\":\n",
    "    version = releases[-1]\n",
    "if version not in releases:\n",
    "    print(f\"Version {version} not found\")\n",
    "    print(f\"Available versions: {releases}\")\n",
    "\n",
    "hdfs_dir_data_files = spark._jvm.org.apache.hadoop.fs.Path(\n",
    "    dir_data.joinpath(version).as_posix()\n",
    ")\n",
    "hdfs_dir_parquet = spark._jvm.org.apache.hadoop.fs.Path(dir_parquet.as_posix())\n",
    "hdfs_dir_version = spark._jvm.org.apache.hadoop.fs.Path(\n",
    "    dir_parquet.joinpath(version).as_posix()\n",
    ")\n",
    "\n",
    "# Create output directories if they do not exist\n",
    "# !hadoop dfs ...\n",
    "# !hadoop dfs -put 20220201 /export/ml4ds/IntelComp/Datalake/SemanticScholar/\n",
    "\n",
    "if not fs.exists(hdfs_dir_parquet):\n",
    "    fs.mkdirs(hdfs_dir_parquet)\n",
    "\n",
    "if not fs.exists(hdfs_dir_version):\n",
    "    fs.mkdirs(hdfs_dir_version)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d598a2d",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9359870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    \"\"\"\n",
    "    Removes extra spaces in text\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_pdf(pdf_list):\n",
    "    \"\"\"\n",
    "    Gets the first valid pdf url for a paper\n",
    "    \"\"\"\n",
    "    pdf_list = [pdf for pdf in pdf_list if pdf.endswith(\".pdf\")]\n",
    "    if len(pdf_list) > 0:\n",
    "        return pdf_list[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "#\n",
    "# Create user defined functions to apply in dataframes\n",
    "#\n",
    "\n",
    "# Obtain ID from author\n",
    "take_id = F.udf(lambda x: normalize(x[0] if len(x) > 0 else None), StringType())\n",
    "\n",
    "# For each paper get all authors\n",
    "take_authors_ids = F.udf(\n",
    "    lambda x: [normalize(el[0] if len(el) > 0 else None) for el in x],\n",
    "    ArrayType(StringType()),\n",
    ")\n",
    "\n",
    "# Remove extra spaces\n",
    "norm_string = F.udf(normalize, StringType())\n",
    "\n",
    "# Get first valid pdf url\n",
    "get_first_pdf = F.udf(get_pdf, StringType())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27f3c4b",
   "metadata": {},
   "source": [
    "### Read data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "faac2eab-438b-42e8-803e-2e7a01bb75c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of abstracts available: 100048909\n",
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- corpusid: long (nullable = true)\n",
      " |-- openaccessinfo: struct (nullable = true)\n",
      " |    |-- externalids: struct (nullable = true)\n",
      " |    |    |-- ACL: string (nullable = true)\n",
      " |    |    |-- ArXiv: string (nullable = true)\n",
      " |    |    |-- DOI: string (nullable = true)\n",
      " |    |    |-- MAG: string (nullable = true)\n",
      " |    |    |-- PubMedCentral: string (nullable = true)\n",
      " |    |-- license: string (nullable = true)\n",
      " |    |-- status: string (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      "\n",
      "-RECORD 0-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " _corrupt_record | null                                                                                                                                                                                                                                 \n",
      " abstract        | Wspomnienie generala broni Tadeusza Buka, ktory zginąl w katastrofie pod Smolenskiem 10 kwietnia 2010 roku.                                                                                                                          \n",
      " corpusid        | 164091528                                                                                                                                                                                                                            \n",
      " openaccessinfo  | {{null, null, null, 2338212559, null}, null, null, null}                                                                                                                                                                             \n",
      "-RECORD 1-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " _corrupt_record | null                                                                                                                                                                                                                                 \n",
      " abstract        | 评论家刘绪源将《妖怪山》的成功归结于一是制作超越了国内图画书的草率和速成,呈现精品化的创作;二是故事和画面的幻想性引人入胜。2015年初,原创图画书《妖怪山》刷爆朋友圈。\"徘徊在人与妖之间\"的著名儿童幻想小说作家彭懿一直致力于图画书研... \n",
      " corpusid        | 218146333                                                                                                                                                                                                                            \n",
      " openaccessinfo  | {{null, null, null, 2497456668, null}, null, null, null}                                                                                                                                                                             \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dir_abstracts = dir_data.joinpath('abstracts')\n",
    "df_abstracts = spark.read.json('file:///' + dir_abstracts.as_posix()).drop('updated')\n",
    "\n",
    "print('Number of abstracts available:', df_abstracts.count())\n",
    "df_abstracts.printSchema()\n",
    "df_abstracts.show(n=2, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05e07bc0-12d4-4c5a-94e6-a9fff0157850",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers available: 211633059\n",
      "root\n",
      " |-- corpusid: long (nullable = true)\n",
      " |-- externalids: struct (nullable = true)\n",
      " |    |-- ACL: string (nullable = true)\n",
      " |    |-- ArXiv: string (nullable = true)\n",
      " |    |-- CorpusId: string (nullable = true)\n",
      " |    |-- DBLP: string (nullable = true)\n",
      " |    |-- DOI: string (nullable = true)\n",
      " |    |-- MAG: string (nullable = true)\n",
      " |    |-- PubMed: string (nullable = true)\n",
      " |    |-- PubMedCentral: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- referencecount: long (nullable = true)\n",
      " |-- citationcount: long (nullable = true)\n",
      " |-- influentialcitationcount: long (nullable = true)\n",
      " |-- S2fieldsofstudy: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- category: string (nullable = true)\n",
      " |    |    |-- source: string (nullable = true)\n",
      "\n",
      "-RECORD 0-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " corpusid                 | 189035839                                                                                                                                             \n",
      " externalids              | {null, null, 189035839, null, null, 2907264494, null, null}                                                                                           \n",
      " title                    | نقش شبکههای اجتماعی مجازی در راهبردهای یادگیری خودتنظیمی و رشد توانایی آیندهپژوهی دانشجویان                                                           \n",
      " year                     | 2018                                                                                                                                                  \n",
      " referencecount           | 0                                                                                                                                                     \n",
      " citationcount            | 0                                                                                                                                                     \n",
      " influentialcitationcount | 0                                                                                                                                                     \n",
      " S2fieldsofstudy          | null                                                                                                                                                  \n",
      "-RECORD 1-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " corpusid                 | 175627287                                                                                                                                             \n",
      " externalids              | {null, null, 175627287, null, null, 410522659, null, null}                                                                                            \n",
      " title                    | H-30 空調用水搬送システムの効率的な運用方法に関する研究 : (その6)冷却水ポンプの水搬送系シミュレーションの構築およびモデルビルにおける省エネ余地の検討 \n",
      " year                     | 2010                                                                                                                                                  \n",
      " referencecount           | 0                                                                                                                                                     \n",
      " citationcount            | 0                                                                                                                                                     \n",
      " influentialcitationcount | 0                                                                                                                                                     \n",
      " S2fieldsofstudy          | null                                                                                                                                                  \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dir_papers = dir_data.joinpath('papers')\n",
    "df_papers = spark.read.json('file:///' + dir_papers.as_posix()).select('corpusid', 'externalids', 'title', 'year', 'referencecount', 'citationcount', 'influentialcitationcount', 'S2fieldsofstudy')\n",
    "\n",
    "print('Number of papers available:', df_papers.count())\n",
    "df_papers.printSchema()\n",
    "df_papers.show(n=2, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf5525df-ca8c-4a7f-89d2-a5c38bb620c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in dataset: 100048997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------------------------------------------------------------------------------------------------------\n",
      " _corrupt_record          | null                                                                                                                     \n",
      " abstract                 | ABSTRACT We investigated seasonal differences in community structure and activity (leucine incorporation) of the plan... \n",
      " corpusid                 | 100031                                                                                                                   \n",
      " openaccessinfo           | {{null, null, 10.1128/AEM.01089-06, 2137258175, null}, null, null, null}                                                 \n",
      " externalids              | {null, null, 100031, null, 10.1128/AEM.01089-06, 2137258175, 17021206, null}                                             \n",
      " title                    | Blooms of Single Bacterial Species in a Coastal Lagoon of the Southwestern Atlantic Ocean                                \n",
      " year                     | 2006                                                                                                                     \n",
      " referencecount           | 72                                                                                                                       \n",
      " citationcount            | 77                                                                                                                       \n",
      " influentialcitationcount | 9                                                                                                                        \n",
      " S2fieldsofstudy          | [{Environmental Science, s2-fos-model}, {Biology, external}, {Medicine, external}]                                       \n",
      "-RECORD 1--------------------------------------------------------------------------------------------------------------------------------------------\n",
      " _corrupt_record          | null                                                                                                                     \n",
      " abstract                 | With the rapidly increasing demands for high-speed and high-density electronic products, complexity and size of the a... \n",
      " corpusid                 | 274189                                                                                                                   \n",
      " openaccessinfo           | {{null, null, 10.1109/TVLSI.2013.2295358, 1970179340, null}, null, null, null}                                           \n",
      " externalids              | {null, null, 274189, journals/tvlsi/PaulANN14, 10.1109/TVLSI.2013.2295358, 1970179340, null, null}                       \n",
      " title                    | Addressing Partitioning Issues in Parallel Circuit Simulation                                                            \n",
      " year                     | 2014                                                                                                                     \n",
      " referencecount           | 12                                                                                                                       \n",
      " citationcount            | 4                                                                                                                        \n",
      " influentialcitationcount | 0                                                                                                                        \n",
      " S2fieldsofstudy          | [{Computer Science, s2-fos-model}, {Computer Science, external}]                                                         \n",
      "-RECORD 2--------------------------------------------------------------------------------------------------------------------------------------------\n",
      " _corrupt_record          | null                                                                                                                     \n",
      " abstract                 | Near-field focused (NFF) antennas can significantly improve wireless system performance with respect to conventional ... \n",
      " corpusid                 | 362657                                                                                                                   \n",
      " openaccessinfo           | {{null, null, 10.1109/ICEAA.2011.6046442, 2082625263, null}, null, null, null}                                           \n",
      " externalids              | {null, null, 362657, null, 10.1109/ICEAA.2011.6046442, 2082625263, null, null}                                           \n",
      " title                    | Near-field focused planar microstrip arrays                                                                              \n",
      " year                     | 2011                                                                                                                     \n",
      " referencecount           | 15                                                                                                                       \n",
      " citationcount            | 1                                                                                                                        \n",
      " influentialcitationcount | 0                                                                                                                        \n",
      " S2fieldsofstudy          | [{Business, s2-fos-model}, {Physics, external}]                                                                          \n",
      "-RECORD 3--------------------------------------------------------------------------------------------------------------------------------------------\n",
      " _corrupt_record          | null                                                                                                                     \n",
      " abstract                 | In recent years, there has been a growth of investigation into the use of artificial immune system as a source of ins... \n",
      " corpusid                 | 399172                                                                                                                   \n",
      " openaccessinfo           | {{null, null, 10.1109/CCDC.2009.5195058, 2133029540, null}, null, null, null}                                            \n",
      " externalids              | {null, null, 399172, null, 10.1109/CCDC.2009.5195058, 2133029540, null, null}                                            \n",
      " title                    | Fault diagnosis algorithm based on artificial immune mechanism                                                           \n",
      " year                     | 2009                                                                                                                     \n",
      " referencecount           | 9                                                                                                                        \n",
      " citationcount            | 1                                                                                                                        \n",
      " influentialcitationcount | 0                                                                                                                        \n",
      " S2fieldsofstudy          | [{Computer Science, s2-fos-model}, {Computer Science, external}]                                                         \n",
      "-RECORD 4--------------------------------------------------------------------------------------------------------------------------------------------\n",
      " _corrupt_record          | null                                                                                                                     \n",
      " abstract                 | Anagrelide represents a treatment option for essential thrombocythemia patients. It lowers platelet counts through in... \n",
      " corpusid                 | 445242                                                                                                                   \n",
      " openaccessinfo           | {{null, null, 10.1111/jth.12850, 1979831105, null}, elsevier-specific: oa user license, BRONZE, http://www.jthjournal... \n",
      " externalids              | {null, null, 445242, null, 10.1111/jth.12850, 1979831105, 25604267, null}                                                \n",
      " title                    | Anagrelide platelet‐lowering effect is due to inhibition of both megakaryocyte maturation and proplatelet formation: ... \n",
      " year                     | 2015                                                                                                                     \n",
      " referencecount           | 34                                                                                                                       \n",
      " citationcount            | 29                                                                                                                       \n",
      " influentialcitationcount | 0                                                                                                                        \n",
      " S2fieldsofstudy          | [{Biology, s2-fos-model}, {Biology, external}, {Medicine, external}]                                                     \n",
      "-RECORD 5--------------------------------------------------------------------------------------------------------------------------------------------\n",
      " _corrupt_record          | null                                                                                                                     \n",
      " abstract                 | The theoretical expression is presented in this study to predict the early minus late power (EMLP) code tracking perf... \n",
      " corpusid                 | 949011                                                                                                                   \n",
      " openaccessinfo           | {{null, null, 10.1109/ICIME.2010.5477657, 2031706092, null}, null, null, null}                                           \n",
      " externalids              | {null, null, 949011, null, 10.1109/ICIME.2010.5477657, 2031706092, null, null}                                           \n",
      " title                    | Code tracking performance of DS/FH spread spectrum signal for TT&C                                                       \n",
      " year                     | 2010                                                                                                                     \n",
      " referencecount           | 14                                                                                                                       \n",
      " citationcount            | 6                                                                                                                        \n",
      " influentialcitationcount | 0                                                                                                                        \n",
      " S2fieldsofstudy          | [{Computer Science, s2-fos-model}, {Computer Science, external}]                                                         \n",
      "-RECORD 6--------------------------------------------------------------------------------------------------------------------------------------------\n",
      " _corrupt_record          | null                                                                                                                     \n",
      " abstract                 | Chronic periodontitis is associated with reduced immunologic reactivity of the body, that may present as disturbed st... \n",
      " corpusid                 | 1000061                                                                                                                  \n",
      " openaccessinfo           | {{null, null, null, null, null}, null, null, null}                                                                       \n",
      " externalids              | {null, null, 1000061, null, null, null, 1412538, null}                                                                   \n",
      " title                    | [The ATPase content of the blood neutrophils and lymphocytes in patients with chronic periodontitis].                    \n",
      " year                     | 1992                                                                                                                     \n",
      " referencecount           | 0                                                                                                                        \n",
      " citationcount            | 0                                                                                                                        \n",
      " influentialcitationcount | 0                                                                                                                        \n",
      " S2fieldsofstudy          | [{Medicine, s2-fos-model}, {Biology, s2-fos-model}, {Medicine, external}]                                                \n",
      "-RECORD 7--------------------------------------------------------------------------------------------------------------------------------------------\n",
      " _corrupt_record          | null                                                                                                                     \n",
      " abstract                 | Prednisolone metasulphabenzoate, a steroid with poor colonic absorption, was coated with the pH‐dependent acrylic res... \n",
      " corpusid                 | 1261800                                                                                                                  \n",
      " openaccessinfo           | {{null, null, 10.1111/j.1365-2036.1992.tb00542.x, 2126686606, null}, null, null, null}                                   \n",
      " externalids              | {null, null, 1261800, null, 10.1111/j.1365-2036.1992.tb00542.x, 2126686606, 1543815, null}                               \n",
      " title                    | An Eudragit‐coated prednisolone preparation for ulcerative colitis: pharmacokinetics and preliminary therapeutic use     \n",
      " year                     | 1992                                                                                                                     \n",
      " referencecount           | 15                                                                                                                       \n",
      " citationcount            | 26                                                                                                                       \n",
      " influentialcitationcount | 1                                                                                                                        \n",
      " S2fieldsofstudy          | [{Medicine, s2-fos-model}, {Medicine, external}]                                                                         \n",
      "-RECORD 8--------------------------------------------------------------------------------------------------------------------------------------------\n",
      " _corrupt_record          | null                                                                                                                     \n",
      " abstract                 | A structured procedure using the judgments of a representative group of local providers for establishing priorities f... \n",
      " corpusid                 | 1420010                                                                                                                  \n",
      " openaccessinfo           | {{null, null, 10.1097/00005650-197811000-00003, 2043409797, null}, null, null, null}                                     \n",
      " externalids              | {null, null, 1420010, null, 10.1097/00005650-197811000-00003, 2043409797, 101722, null}                                  \n",
      " title                    | Priority Setting in Quality Assurance: Reliability of Staff Judgments in Medical Institutions                            \n",
      " year                     | 1978                                                                                                                     \n",
      " referencecount           | 0                                                                                                                        \n",
      " citationcount            | 16                                                                                                                       \n",
      " influentialcitationcount | 0                                                                                                                        \n",
      " S2fieldsofstudy          | [{Medicine, s2-fos-model}, {Medicine, external}]                                                                         \n",
      "-RECORD 9--------------------------------------------------------------------------------------------------------------------------------------------\n",
      " _corrupt_record          | null                                                                                                                     \n",
      " abstract                 | Eulerian numbers (and ''Alternate Eulerian numbers'') are often interpreted as distributions of statistics defined ov... \n",
      " corpusid                 | 1655862                                                                                                                  \n",
      " openaccessinfo           | {{null, null, 10.46298/dmtcs.271, 1517544418, null}, CCBY, GOLD, https://dmtcs.episciences.org/271/pdf}                  \n",
      " externalids              | {null, null, 1655862, journals/dmtcs/MantaciR01, 10.46298/dmtcs.271, 1517544418, null, null}                             \n",
      " title                    | A permutations representation that knows what \"Eulerian\" means                                                           \n",
      " year                     | 2001                                                                                                                     \n",
      " referencecount           | 14                                                                                                                       \n",
      " citationcount            | 36                                                                                                                       \n",
      " influentialcitationcount | 3                                                                                                                        \n",
      " S2fieldsofstudy          | [{Mathematics, s2-fos-model}, {Computer Science, external}, {Mathematics, external}]                                     \n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset = (df_abstracts.join(df_papers, df_abstracts.corpusid ==  df_papers.corpusid, \"left\")\n",
    "                      .drop(df_papers.corpusid)\n",
    "                )\n",
    "\n",
    "print('Number of documents in dataset:', dataset.count())\n",
    "dataset.show(n=10, truncate=120, vertical=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a808c5dc-e506-45c9-93c0-0e8cfdc5b0c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset.coalesce(1000).write.parquet(\n",
    "    dir_parquet.joinpath(f\"papers_IMT.parquet\").as_posix(),\n",
    "    mode=\"overwrite\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7eb16021-f4c6-4f42-a0d1-2277bd17e3b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 66:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------------------------------------------------------------\n",
      " externalids     | {null, null, 176711, null, 10.1590/S0004-28032015000100014, 1957784724, 26017086, null} \n",
      " S2fieldsofstudy | [{Biology, s2-fos-model}, {Medicine, s2-fos-model}, {Medicine, external}]               \n",
      "-RECORD 1--------------------------------------------------------------------------------------------------\n",
      " externalids     | {null, null, 290964, null, null, 2407907656, 25185378, null}                            \n",
      " S2fieldsofstudy | [{Medicine, s2-fos-model}, {Medicine, external}]                                        \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset_small = dataset.sample(fraction=0.001).select(\"externalids\", \"S2fieldsofstudy\")\n",
    "dataset_small.show(n=2, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b2ba5d8-4da6-4df3-8ef3-c4f0d7d88d7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfPersist = dataset_small.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d3067c7-021f-4888-9df8-ae5fed1445a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.               (0 + 13) / 31]\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.4.0-bin-3.3.1/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark-3.4.0-bin-3.3.1/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdfPersist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark-3.4.0-bin-3.3.1/python/pyspark/sql/dataframe.py:912\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    905\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    906\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m         },\n\u001b[1;32m    910\u001b[0m     )\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/spark-3.4.0-bin-3.3.1/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark-3.4.0-bin-3.3.1/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/spark-3.4.0-bin-3.3.1/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 67:====>          (10 + 21) / 31][Stage 69:>               (0 + 19) / 31]\r"
     ]
    }
   ],
   "source": [
    "dfPersist.show(n=2, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26878e1-7e00-4007-88dc-c33214e23f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_FOS = F.udf(lambda x:list(x), StringType() )\n",
    "dataset_small = (\n",
    "    dataset_small.withColumn(\"FOS\",udf_FOS(F.col(\"S2fieldsofstudy\")))\n",
    ")\n",
    "\n",
    "dataset_small.show(n=2, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab19392-a063-41ba-90c5-c8499a38c4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5e773b-e873-40c8-a227-2f004f83afdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cacb29-bb92-4363-824e-eebadb106291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38bc574-656f-4faf-93fd-92d0b2364b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c387046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/09 23:54:15 WARN datasources.SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.9 s, sys: 279 ms, total: 2.18 s\n",
      "Wall time: 10min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Read data files\n",
    "#\n",
    "# Create a spark df with all the papers in all json files\n",
    "\n",
    "df = spark.read.json(dir_data.joinpath(version).as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c662d1e3",
   "metadata": {},
   "source": [
    "### Create papers dataframe and save as parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6e36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create papers dataframe and save as parquet file\n",
    "#\n",
    "# Papers table will be created keeping only a subset of desired columns\n",
    "# It is then stored in disk as a parquet file\n",
    "\n",
    "# Columns to save\n",
    "columns = [\n",
    "    \"id\",\n",
    "    \"title\",\n",
    "    \"paperAbstract\",\n",
    "    \"s2Url\",\n",
    "    \"pdfUrls\",\n",
    "    \"year\",\n",
    "    \"sources\",\n",
    "    \"doi\",\n",
    "    \"doiUrl\",\n",
    "    \"pmid\",\n",
    "    \"magId\",\n",
    "    \"fieldsOfStudy\",\n",
    "    \"journalName\",\n",
    "    \"journalPages\",\n",
    "    \"journalVolume\",\n",
    "    \"venue\",\n",
    "]\n",
    "# Select papers info\n",
    "df_papers = df.select(columns)\n",
    "\n",
    "# Clean info\n",
    "for c in columns:\n",
    "    if df.select(c).dtypes[0][1] == \"string\":\n",
    "        df_papers = df_papers.withColumn(c, norm_string(c))\n",
    "\n",
    "# Save dataframe as parquet\n",
    "df_papers.write.parquet(\n",
    "    dir_parquet.joinpath(version).joinpath(\"papers.parquet\").as_posix(),\n",
    "    mode=\"overwrite\",\n",
    ")\n",
    "\n",
    "print('Number of papers in S2 version ' + version + ':', df_papers.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e528dff6",
   "metadata": {},
   "source": [
    "### Create authors dataframe and save as parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41da632",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create authors dataframe and save as parquet file\n",
    "#\n",
    "# Authors table will be created from all authors listed in every paper\n",
    "# - Duplicates will be removed keeping only one row for each author id\n",
    "# - Authors with empty ids will also be removed from dataframe\n",
    "\n",
    "# Select only the authors\n",
    "df_authors = df.select(F.explode(\"authors\").alias(\"authors\"))\n",
    "\n",
    "# Convert dataframe into two columns (id, author name)\n",
    "df_authors = (\n",
    "    df_authors.select(\"authors.ids\", \"authors.name\")\n",
    "    .withColumn(\"ids\", take_id(\"ids\"))\n",
    "    .withColumn(\"name\", norm_string(\"name\"))\n",
    "    .withColumnRenamed(\"ids\", \"id\")\n",
    "    .drop_duplicates(subset=[\"id\"])\n",
    "    .dropna(subset=[\"id\"])\n",
    ")\n",
    "\n",
    "# Save dataframe as parquet\n",
    "df_authors.write.parquet(\n",
    "    dir_parquet.joinpath(version).joinpath(\"authors.parquet\").as_posix(), mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "print('Number of authors in S2 version ' + version + ':', df_authors.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5633e9f4",
   "metadata": {},
   "source": [
    "### Create citations dataframe and save as parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4ec063",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create citations dataframe and save as parquet file\n",
    "#\n",
    "# We create a row paper_source_id -> paper_destination_id\n",
    "# by exploding all citations of all papers in the version\n",
    "\n",
    "# Select paper-authors info\n",
    "df_citations = df.select([\"id\", \"outCitations\"])\n",
    "df_citations = (\n",
    "    df_citations.withColumn(\"outCitations\", F.explode(\"outCitations\"))\n",
    "    .withColumnRenamed(\"id\", \"source\")\n",
    "    .withColumnRenamed(\"outCitations\", \"dest\")\n",
    ")\n",
    "\n",
    "# Save dataframe as parquet\n",
    "df_citations.write.parquet(\n",
    "    dir_parquet.joinpath(version).joinpath(\"citations.parquet\").as_posix(),\n",
    "    mode=\"overwrite\",\n",
    ")\n",
    "\n",
    "print('Number of citations in S2 version ' + version + ':', df_citations.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293c3b4e",
   "metadata": {},
   "source": [
    "### Create paper_author dataframe and save as parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59b5e556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of authorships in S2 version 20220201: 551656008\n",
      "CPU times: user 4.01 s, sys: 714 ms, total: 4.72 s\n",
      "Wall time: 21min 10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create paper_author dataframe and save as parquet file\n",
    "#\n",
    "# We create a row paper_id -> author_id\n",
    "# by exploding all authors of all papers in the version\n",
    "\n",
    "# Select paper-authors info\n",
    "df_paperAuthor = df.select([\"id\", \"authors\"])\n",
    "df_paperAuthor = df_paperAuthor.dropna()\n",
    "df_paperAuthor = (\n",
    "    df_paperAuthor.withColumn(\"authors\", F.explode(take_authors_ids(\"authors.ids\")))\n",
    "    .withColumnRenamed(\"id\", \"paper_id\")\n",
    "    .withColumnRenamed(\"authors\", \"author_id\")\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "# Save dataframe as parquet\n",
    "df_paperAuthor.write.parquet(\n",
    "    dir_parquet.joinpath(version).joinpath(\"paper_author.parquet\").as_posix(),\n",
    "    mode=\"overwrite\",\n",
    ")\n",
    "\n",
    "print('Number of authorships in S2 version ' + version + ':', df_paperAuthor.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c481351",
   "metadata": {},
   "source": [
    "### Download PDFs (IN PROGRESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c6fa0a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/export/ml4ds/IntelComp/Datalake/SemanticScholar/rawdata/20220201/pdfs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12341/439138226.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get previously downloaded pdfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m list_pdfs = set(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir_pdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12341/439138226.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get previously downloaded pdfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m list_pdfs = set(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir_pdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/pathlib.py\u001b[0m in \u001b[0;36miterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'..'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m                 \u001b[0;31m# Yielding a path object for these makes little sense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/export/ml4ds/IntelComp/Datalake/SemanticScholar/rawdata/20220201/pdfs'"
     ]
    }
   ],
   "source": [
    "# Get previously downloaded pdfs\n",
    "list_pdfs = set(\n",
    "    [x.stem for x in dir_pdfs.iterdir() if x.is_file()]\n",
    ")\n",
    "\n",
    "# Select pdfs to download\n",
    "pdf_urls = (\n",
    "    df.select([\"id\", \"pdfUrls\"])\n",
    "    .withColumn(\"pdfUrls\", get_first_pdf(\"pdfUrls\"))\n",
    "    .filter(F.length(\"pdfUrls\") > 0)\n",
    ")\n",
    "pdf_urls = pdf_urls.where(~F.col(\"id\").isin(list_pdfs))\n",
    "\n",
    "pdf_test = pdf_urls.limit(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55995cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download PDFs\n",
    "#\n",
    "# We download PDFs for all papers with valid a valid pdfUrl\n",
    "# This option is not activated by default, since the number\n",
    "# of papers to download would be huge\n",
    "paper_download = 1\n",
    "\n",
    "if paper_download:\n",
    "\n",
    "    def download_pdfs(row, dir_pdfs=dir_pdfs):\n",
    "        try:\n",
    "            r = requests.get(row[\"pdfUrls\"], stream=True)\n",
    "            with dir_pdfs.joinpath(f\"{row['id']}.pdf\").open(\"wb\") as f:\n",
    "                f.write(r.content)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    pdf_test.foreach(download_pdfs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
