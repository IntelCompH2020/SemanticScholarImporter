{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e420d98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47791f9f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e00d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "from pathlib import Path\n",
    "import pyspark.sql.functions as F\n",
    "import requests\n",
    "from pyspark.sql.types import ArrayType, StringType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fbb4ea",
   "metadata": {},
   "source": [
    "### Define directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e75ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "#\n",
    "# Relevant directories are read from the config file:\n",
    "# dir_data:    full path to hdfs directory where the raw data .gz files are stored\n",
    "# dir_parquet: full path to hdfs directory where the parquet tables will be stored\n",
    "# version:     Version of Semantic Scholar that is being processed\n",
    "#              for information purposes only\n",
    "\n",
    "cf = ConfigParser()\n",
    "cf.read(\"../config.cf\")\n",
    "\n",
    "dir_data = Path(cf.get(\"spark\", \"dir_data\"))\n",
    "dir_parquet = Path(cf.get(\"spark\", \"dir_parquet\"))\n",
    "version = cf.get(\"spark\", \"version\")\n",
    "dir_pdfs = Path(cf.get(\"spark\", \"dir_pdfs\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b88ac9d",
   "metadata": {},
   "source": [
    "#### Configuration hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1c1fc6",
   "metadata": {},
   "source": [
    "It is not possible to listdir() directly using Path as it is a hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be032487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration hdfs\n",
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "hdfs_dir_data = spark._jvm.org.apache.hadoop.fs.Path(dir_data.as_posix())\n",
    "\n",
    "# Get selected version\n",
    "releases = sorted(\n",
    "    [\n",
    "        f.getPath().getName()\n",
    "        for f in fs.listStatus(hdfs_dir_data)\n",
    "        if f.isDirectory() and f.getPath().getName().isdigit()\n",
    "    ]\n",
    ")\n",
    "version = version.replace(\"-\", \"\")\n",
    "if version == \"last\":\n",
    "    version = releases[-1]\n",
    "if version not in releases:\n",
    "    print(f\"Version {version} not found\")\n",
    "    print(f\"Available versions: {releases}\")\n",
    "\n",
    "hdfs_dir_data_files = spark._jvm.org.apache.hadoop.fs.Path(\n",
    "    dir_data.joinpath(version).as_posix()\n",
    ")\n",
    "hdfs_dir_parquet = spark._jvm.org.apache.hadoop.fs.Path(dir_parquet.as_posix())\n",
    "hdfs_dir_version = spark._jvm.org.apache.hadoop.fs.Path(\n",
    "    dir_parquet.joinpath(version).as_posix()\n",
    ")\n",
    "\n",
    "# Create output directories if they do not exist\n",
    "# !hadoop dfs ...\n",
    "# !hadoop dfs -put 20220201 /export/ml4ds/IntelComp/Datalake/SemanticScholar/\n",
    "\n",
    "if not fs.exists(hdfs_dir_parquet):\n",
    "    fs.mkdirs(hdfs_dir_parquet)\n",
    "\n",
    "if not fs.exists(hdfs_dir_version):\n",
    "    fs.mkdirs(hdfs_dir_version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d598a2d",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9359870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    \"\"\"\n",
    "    Removes extra spaces in text\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_pdf(pdf_list):\n",
    "    \"\"\"\n",
    "    Gets the first valid pdf url for a paper\n",
    "    \"\"\"\n",
    "    pdf_list = [pdf for pdf in pdf_list if pdf.endswith(\".pdf\")]\n",
    "    if len(pdf_list) > 0:\n",
    "        return pdf_list[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "#\n",
    "# Create user defined functions to apply in dataframes\n",
    "#\n",
    "\n",
    "# Obtain ID from author\n",
    "take_id = F.udf(lambda x: normalize(x[0] if len(x) > 0 else None), StringType())\n",
    "\n",
    "# For each paper get all authors\n",
    "take_authors_ids = F.udf(\n",
    "    lambda x: [normalize(el[0] if len(el) > 0 else None) for el in x],\n",
    "    ArrayType(StringType()),\n",
    ")\n",
    "\n",
    "# Remove extra spaces\n",
    "norm_string = F.udf(normalize, StringType())\n",
    "\n",
    "# Get first valid pdf url\n",
    "get_first_pdf = F.udf(get_pdf, StringType())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27f3c4b",
   "metadata": {},
   "source": [
    "### Read data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c387046",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Read data files\n",
    "#\n",
    "# Create a spark df with all the papers in all json files\n",
    "\n",
    "df = spark.read.json(dir_data.joinpath(version).as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c662d1e3",
   "metadata": {},
   "source": [
    "### Create papers dataframe and save as parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6e36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create papers dataframe and save as parquet file\n",
    "#\n",
    "# Papers table will be created keeping only a subset of desired columns\n",
    "# It is then stored in disk as a parquet file\n",
    "\n",
    "# Columns to save\n",
    "columns = [\n",
    "    \"id\",\n",
    "    \"title\",\n",
    "    \"paperAbstract\",\n",
    "    \"s2Url\",\n",
    "    \"pdfUrls\",\n",
    "    \"year\",\n",
    "    \"sources\",\n",
    "    \"doi\",\n",
    "    \"doiUrl\",\n",
    "    \"pmid\",\n",
    "    \"magId\",\n",
    "    \"fieldsOfStudy\",\n",
    "    \"journalName\",\n",
    "    \"journalPages\",\n",
    "    \"journalVolume\",\n",
    "    \"venue\",\n",
    "]\n",
    "# Select papers info\n",
    "df_papers = df.select(columns)\n",
    "\n",
    "# Clean info\n",
    "for c in columns:\n",
    "    if df.select(c).dtypes[0][1] == \"string\":\n",
    "        df_papers = df_papers.withColumn(c, norm_string(c))\n",
    "\n",
    "# Save dataframe as parquet\n",
    "df_papers.write.parquet(\n",
    "    dir_parquet.joinpath(version).joinpath(\"papers.parquet\").as_posix(),\n",
    "    mode=\"overwrite\",\n",
    ")\n",
    "\n",
    "print('Number of papers in S2 version ' + version + ':', df_papers.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e528dff6",
   "metadata": {},
   "source": [
    "### Create authors dataframe and save as parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41da632",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create authors dataframe and save as parquet file\n",
    "#\n",
    "# Authors table will be created from all authors listed in every paper\n",
    "# - Duplicates will be removed keeping only one row for each author id\n",
    "# - Authors with empty ids will also be removed from dataframe\n",
    "\n",
    "# Select only the authors\n",
    "df_authors = df.select(F.explode(\"authors\").alias(\"authors\"))\n",
    "\n",
    "# Convert dataframe into two columns (id, author name)\n",
    "df_authors = (\n",
    "    df_authors.select(\"authors.ids\", \"authors.name\")\n",
    "    .withColumn(\"ids\", take_id(\"ids\"))\n",
    "    .withColumn(\"name\", norm_string(\"name\"))\n",
    "    .withColumnRenamed(\"ids\", \"id\")\n",
    "    .drop_duplicates(subset=[\"id\"])\n",
    "    .dropna(subset=[\"id\"])\n",
    ")\n",
    "\n",
    "# Save dataframe as parquet\n",
    "df_authors.write.parquet(\n",
    "    dir_parquet.joinpath(version).joinpath(\"authors.parquet\").as_posix(), mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "print('Number of authors in S2 version ' + version + ':', df_authors.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5633e9f4",
   "metadata": {},
   "source": [
    "### Create citations dataframe and save as parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4ec063",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create citations dataframe and save as parquet file\n",
    "#\n",
    "# We create a row paper_source_id -> paper_destination_id\n",
    "# by exploding all citations of all papers in the version\n",
    "\n",
    "# Select paper-authors info\n",
    "df_citations = df.select([\"id\", \"outCitations\"])\n",
    "df_citations = (\n",
    "    df_citations.withColumn(\"outCitations\", F.explode(\"outCitations\"))\n",
    "    .withColumnRenamed(\"id\", \"source\")\n",
    "    .withColumnRenamed(\"outCitations\", \"dest\")\n",
    ")\n",
    "\n",
    "# Save dataframe as parquet\n",
    "df_citations.write.parquet(\n",
    "    dir_parquet.joinpath(version).joinpath(\"citations.parquet\").as_posix(),\n",
    "    mode=\"overwrite\",\n",
    ")\n",
    "\n",
    "print('Number of citations in S2 version ' + version + ':', df_citations.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293c3b4e",
   "metadata": {},
   "source": [
    "### Create paper_author dataframe and save as parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b5e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create paper_author dataframe and save as parquet file\n",
    "#\n",
    "# We create a row paper_id -> author_id\n",
    "# by exploding all authors of all papers in the version\n",
    "\n",
    "# Select paper-authors info\n",
    "df_paperAuthor = df.select([\"id\", \"authors\"])\n",
    "df_paperAuthor = df_paperAuthor.dropna()\n",
    "df_paperAuthor = (\n",
    "    df_paperAuthor.withColumn(\"authors\", F.explode(take_authors_ids(\"authors.ids\")))\n",
    "    .withColumnRenamed(\"id\", \"paper_id\")\n",
    "    .withColumnRenamed(\"authors\", \"author_id\")\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "# Save dataframe as parquet\n",
    "df_paperAuthor.write.parquet(\n",
    "    dir_parquet.joinpath(version).joinpath(\"paper_author.parquet\").as_posix(),\n",
    "    mode=\"overwrite\",\n",
    ")\n",
    "\n",
    "print('Number of authorships in S2 version ' + version + ':', df_paperAuthor.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c481351",
   "metadata": {},
   "source": [
    "### Download PDFs (IN PROGRESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6fa0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get previously downloaded pdfs\n",
    "list_pdfs = set(\n",
    "    [x.stem for x in dir_pdfs.iterdir() if x.is_file()]\n",
    ")\n",
    "\n",
    "# Select pdfs to download\n",
    "pdf_urls = (\n",
    "    df.select([\"id\", \"pdfUrls\"])\n",
    "    .withColumn(\"pdfUrls\", get_first_pdf(\"pdfUrls\"))\n",
    "    .filter(F.length(\"pdfUrls\") > 0)\n",
    ")\n",
    "pdf_urls = pdf_urls.where(~F.col(\"id\").isin(list_pdfs))\n",
    "\n",
    "pdf_test = pdf_urls.limit(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55995cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download PDFs\n",
    "#\n",
    "# We download PDFs for all papers with valid a valid pdfUrl\n",
    "# This option is not activated by default, since the number\n",
    "# of papers to download would be huge\n",
    "paper_download = 1\n",
    "\n",
    "if paper_download:\n",
    "\n",
    "    def download_pdfs(row, dir_pdfs=dir_pdfs):\n",
    "        try:\n",
    "            r = requests.get(row[\"pdfUrls\"], stream=True)\n",
    "            with dir_pdfs.joinpath(f\"{row['id']}.pdf\").open(\"wb\") as f:\n",
    "                f.write(r.content)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    pdf_test.foreach(download_pdfs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
